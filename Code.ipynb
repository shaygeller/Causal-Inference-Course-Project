{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_name = \"s_2018_all_raw.xlsx\"\n",
    "data = pd.read_excel(file_name)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove comments at the end of the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from NB\n",
    "lectures_due_dates = {'bio2018_lecture01':'2018-04-09',\n",
    " 'bio2018_lecture02':'2018-04-09',\n",
    " 'bio2018_lecture03':'2018-04-09',\n",
    " 'bio2018_lecture04':'2018-04-09',\n",
    " 'bio2018_lecture05':'2018-04-11',\n",
    " 'bio2018_lecture06':'2018-04-13',\n",
    " 'bio2018_lecture07':'2018-04-16',\n",
    " 'bio2018_lecture08':'2018-04-18',\n",
    " 'bio2018_lecture09':'2018-04-20',\n",
    " 'bio2018_lecture10':'2018-04-23',\n",
    " 'bio2018_lecture11':'2018-04-25',\n",
    " 'bio2018_lecture12':'2018-04-30',\n",
    " 'bio2018_lecture13':'2018-05-02',\n",
    " 'bio2018_lecture14':'2018-05-04',\n",
    " 'bio2018_lecture15':'2018-05-07',\n",
    " 'bio2018_lecture16':'2018-05-09',\n",
    " 'bio2018_lecture17':'2018-05-11',\n",
    " 'bio2018_lecture18':'2018-05-18',\n",
    " 'bio2018_lecture20':'2018-05-21',\n",
    " 'bio2018_lecture21':'2018-05-23',\n",
    " 'bio2018_lecture22':'2018-05-25',\n",
    " 'bio2018_lecture23':'2018-05-30',\n",
    " 'bio2018_lecture24':'2018-06-01',\n",
    " 'bio2018_lecture25':'2018-06-04',\n",
    " 'bio2018_lecture26':'2018-06-06'}\n",
    "\n",
    "\n",
    "lectures_due_hours = {'bio2018_lecture01':10,\n",
    " 'bio2018_lecture02':10,\n",
    " 'bio2018_lecture03':10,\n",
    " 'bio2018_lecture04':22,\n",
    " 'bio2018_lecture05':22,\n",
    " 'bio2018_lecture06':22,\n",
    " 'bio2018_lecture07':22,\n",
    " 'bio2018_lecture08':22,\n",
    " 'bio2018_lecture09':22,\n",
    " 'bio2018_lecture10':22,\n",
    " 'bio2018_lecture11':22,\n",
    " 'bio2018_lecture12':22,\n",
    " 'bio2018_lecture13':22,\n",
    " 'bio2018_lecture14':22,\n",
    " 'bio2018_lecture15':22,\n",
    " 'bio2018_lecture16':22,\n",
    " 'bio2018_lecture17':22,\n",
    " 'bio2018_lecture18':22,\n",
    " 'bio2018_lecture20':22,\n",
    " 'bio2018_lecture21':22,\n",
    " 'bio2018_lecture22':22,\n",
    " 'bio2018_lecture23':22,\n",
    " 'bio2018_lecture24':22,\n",
    " 'bio2018_lecture25':22,\n",
    " 'bio2018_lecture26':22}\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# Get date and hour columns\n",
    "data[\"date\"] = data[\"ctime\"].str.split(\"T\").str[0]\n",
    "data[\"ctime\"] = data[\"ctime\"].astype(\"datetime64[ns]\")\n",
    "data[\"hour\"] = data[\"ctime\"].dt.hour\n",
    "\n",
    "\n",
    "data_temp = data.copy()\n",
    "df_list = []\n",
    "# Filter comments too close to due date (close=written 5 hours or less before due date)\n",
    "for lec,date in lectures_due_dates.items():\n",
    "    df_list.extend(data_temp[((data_temp[\"Source_lecture\"]==lec) & (data_temp[\"date\"]!=date)) | \n",
    "                                 ((data_temp[\"Source_lecture\"]==lec) & (data_temp[\"date\"]==date) & (data_temp[\"hour\"]<lectures_due_hours[lec]-5)) |\n",
    "                            ((data_temp[\"Source_lecture\"]==lec) & (data_temp[\"date\"]==date) & (data_temp[\"hour\"]>=lectures_due_hours[lec]-5) & (pd.notna(data_temp[\"parent_id\"])))].values.tolist())\n",
    "    \n",
    "data = pd.DataFrame(df_list, columns = list(data))\n",
    "data.head(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove teacher authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teachers_ids = [\"310455\",\"996299\", \"1088779\",\"1089355\"]\n",
    "data = data[~data['author_id'].isin(teachers_ids)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get aggregated data by student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = set(data[\"author_id\"].unique().tolist())\n",
    "lectures = set(data[\"Source_lecture\"].unique().tolist())\n",
    "\n",
    "all_comments_dict = data.groupby([\"author_id\",\"Source_lecture\"])[\"text\"].count().to_dict()\n",
    "\n",
    "first_comments_dict = data[data[\"parent_id\"].isna()].groupby([\"author_id\",\"Source_lecture\"])[\"text\"].count().to_dict()\n",
    "\n",
    "reply_comments_dict = data[~data[\"parent_id\"].isna()].groupby([\"author_id\",\"Source_lecture\"])[\"text\"].count().to_dict()\n",
    "\n",
    "# append zeros when missing\n",
    "for key in all_comments_dict:\n",
    "    if key not in first_comments_dict.keys():\n",
    "        first_comments_dict[key] = 0\n",
    "    if key not in reply_comments_dict.keys():\n",
    "        reply_comments_dict[key] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of comments: {sum(all_comments_dict.values())}\")\n",
    "print(f\"Number of Students: {len(authors)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove students that didn't comment over all of the lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of students before\")\n",
    "print(len(all_comments_dict.keys()))\n",
    "print(len(first_comments_dict.keys()))\n",
    "print(len(reply_comments_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lectures = sorted(list(set(data[\"Source_lecture\"].unique().tolist())))\n",
    "lectures_per_student = data.groupby([\"author_id\"])[\"Source_lecture\"].nunique().to_dict()\n",
    "all_comments_keys = list(all_comments_dict.keys())\n",
    "first_comments_keys = list(first_comments_dict.keys())\n",
    "reply_comments_keys = list(reply_comments_dict.keys())\n",
    "\n",
    "for k in all_comments_keys:\n",
    "    if lectures_per_student[k[0]] != 25:\n",
    "        del all_comments_dict[k]\n",
    "        if k in first_comments_keys:\n",
    "            del first_comments_dict[k]\n",
    "        if k in reply_comments_keys:\n",
    "            del reply_comments_dict[k]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of students after\")\n",
    "print(len(all_comments_dict.keys()))\n",
    "print(len(first_comments_dict.keys()))\n",
    "print(len(reply_comments_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get number of comments in the next lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = sorted(list(all_comments_dict.keys()))\n",
    "good_authors = list(set(k[0] for k in all_keys))\n",
    "\n",
    "next_all_comments_dict = {}\n",
    "for curr_l, next_l in zip(lectures, lectures[1:]):\n",
    "    for author in good_authors:\n",
    "        next_all_comments_dict[(author,curr_l)] = all_comments_dict[(author,next_l)]\n",
    "        if next_l == \"bio2018_lecture26\":\n",
    "            next_all_comments_dict[(author,next_l)] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of Comments: {sum(all_comments_dict.values())}\")\n",
    "print(f\"Number of Students: {len(good_authors)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get treatment label\n",
    "Treatment = At least one of the student's posts got answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thread_length_dict = data.groupby([\"location_id\"])[\"text\"].count().to_dict()\n",
    "data[\"got_answered\"] = data.apply(lambda row: 1 if (thread_length_dict[row[\"location_id\"]]>1 and pd.isnull(row[\"parent_id\"])) else 0, axis=1)\n",
    "got_answered_dict = data.groupby([\"author_id\",\"Source_lecture\"])[\"got_answered\"].sum().to_dict()\n",
    "\n",
    "# Change to binary value (1=Got answered, 0=Not answered)\n",
    "for k,v in got_answered_dict.items():\n",
    "    if v>0:\n",
    "        got_answered_dict[k] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get verbosity level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].astype(str)\n",
    "student_lecture_verbosity = data.groupby([\"author_id\",\"Source_lecture\"])[\"text\"].apply(lambda x: \"\\n\".join(x)).apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get emoji counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confused_hashtag = data.groupby([\"author_id\",\"Source_lecture\"])[\"#confused\"].sum().to_dict()\n",
    "curious_hashtag = data.groupby([\"author_id\",\"Source_lecture\"])[\"#curious\"].sum().to_dict()\n",
    "help_hashtag = data.groupby([\"author_id\",\"Source_lecture\"])[\"#help\"].sum().to_dict()\n",
    "useful_hashtag = data.groupby([\"author_id\",\"Source_lecture\"])[\"#useful\"].sum().to_dict()\n",
    "interested_hashtag = data.groupby([\"author_id\",\"Source_lecture\"])[\"#interested\"].sum().to_dict()\n",
    "frustrated_hashtag = data.groupby([\"author_id\",\"Source_lecture\"])[\"#frustrated\"].sum().to_dict()\n",
    "question_hashtag = data.groupby([\"author_id\",\"Source_lecture\"])[\"#question\"].sum().to_dict()\n",
    "idea_hashtag = data.groupby([\"author_id\",\"Source_lecture\"])[\"#idea\"].sum().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get section id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_lecture_section_id = data.groupby([\"author_id\",\"Source_lecture\"])[\"section_id\"].max()\n",
    "\n",
    "# Correct for students with NAN value in section id\n",
    "studet_section_id = {k[0]:[] for k in student_lecture_section_id.keys()}\n",
    "for k,v in student_lecture_section_id.items():\n",
    "    studet_section_id[k[0]].append(v)\n",
    "    \n",
    "\n",
    "for k,v in student_lecture_section_id.items():\n",
    "    if pd.isna(v):\n",
    "        student_lecture_section_id[k] = max(studet_section_id[k[0]])\n",
    "        \n",
    "\n",
    "# weird student, this is a manual correction\n",
    "student_lecture_section_id[(1121374, 'bio2018_lecture01')] = 0\n",
    "student_lecture_section_id[(1121374, 'bio2018_lecture02')] = 0\n",
    "student_lecture_section_id[(1121374, 'bio2018_lecture03')] = 0\n",
    "student_lecture_section_id[(1121374, 'bio2018_lecture018')] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create final dataframe\n",
    "Remove lecture 26 - This is the last lecture and doesn't contain Y value(number of comments in the next lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for k,v in all_comments_dict.items():\n",
    "    # Remove lecture 26\n",
    "    if k[1]==\"bio2018_lecture26\":\n",
    "        continue\n",
    "    curr_list = []\n",
    "    curr_list.append(k[0]) # author\n",
    "    curr_list.append(k[1]) # lecture\n",
    "    curr_list.append(v) # Number of comments in current lecture\n",
    "\n",
    "    curr_list.append(first_comments_dict[k]) # Number of first-comments\n",
    "    curr_list.append(reply_comments_dict[k]) # Number of reply-comments\n",
    "    curr_list.append(student_lecture_verbosity[k]) # Student verbosity\n",
    "    curr_list.append(student_lecture_section_id[k]) # Student section id\n",
    "    \n",
    "    curr_list.append(confused_hashtag[k])   \n",
    "    curr_list.append(curious_hashtag[k])   \n",
    "    curr_list.append(help_hashtag[k]) \n",
    "    curr_list.append(useful_hashtag[k])\n",
    "    curr_list.append(interested_hashtag[k])\n",
    "    curr_list.append(frustrated_hashtag[k])\n",
    "    curr_list.append(question_hashtag[k]) \n",
    "    curr_list.append(idea_hashtag[k]) \n",
    "\n",
    "    \n",
    "    \n",
    "    curr_list.append(got_answered_dict[k]) # Is got answered in the current lecture (T)\n",
    "    curr_list.append(next_all_comments_dict[k]) # Number of coments ins the next lecture (Y)\n",
    "    \n",
    "    df_list.append(curr_list)\n",
    "final_df = pd.DataFrame(df_list, columns=[\"author\",\"lecture\",\"#total-comments\",\"#first-comments\",\"#reply-comments\",\n",
    "                                          \"verbosity\",\"section_id\",\"confused_hashtag\", \"curious_hashtag\", \n",
    "                                          \"help_hashtag\", \"useful_hashtag\", \"interested_hashtag\", \n",
    "                                          \"frustrated_hashtag\", \"question_hashtag\", \"idea_hashtag\", \"T\",\"Y\"])\n",
    "final_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe for causal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale numerical values\n",
    "features_df = final_df[[\"#total-comments\",\"#first-comments\",\"#reply-comments\",\"verbosity\", \"confused_hashtag\", \"curious_hashtag\", \"help_hashtag\", \"useful_hashtag\", \"interested_hashtag\", \"frustrated_hashtag\", \"question_hashtag\", \"idea_hashtag\"]]\n",
    "scaler = StandardScaler()\n",
    "features_df_scaled = pd.DataFrame(scaler.fit_transform(features_df))\n",
    "\n",
    "# Get one hot encoding of lectures and authors\n",
    "lectures_df = pd.get_dummies(final_df[\"lecture\"])\n",
    "authors_df = pd.get_dummies(final_df[\"author\"])\n",
    "section_df = pd.get_dummies(final_df[\"section_id\"])\n",
    "\n",
    "# Concat all to a signle dataframe\n",
    "features_df = pd.concat([features_df_scaled, lectures_df,authors_df,section_df], axis=1)    \n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of treated authors (at least once across all lectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "authors_got_answered = 0\n",
    "for k,v in final_df.groupby(\"author\")[\"T\"].sum().to_dict().items():\n",
    "    if v>0:\n",
    "        authors_got_answered+=1\n",
    "print(\"Number of authors: \", len(authors))\n",
    "print(\"Number of authors that got answerd at least once during the semester: \", authors_got_answered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- Add more features from Davis: Age, sex, demographic, First language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check a simple correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df[\"T\"]==0].shape, final_df[final_df[\"T\"]==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df[final_df[\"T\"]==1].groupby(\"lecture\")[\"Y\"].mean().mean())\n",
    "print(final_df[final_df[\"T\"]==1].groupby(\"lecture\")[\"Y\"].mean().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df[final_df[\"T\"]==0].groupby(\"lecture\")[\"Y\"].mean().mean())\n",
    "print(final_df[final_df[\"T\"]==0].groupby(\"lecture\")[\"Y\"].mean().std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check causation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get baseline model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalinference import CausalModel\n",
    "\n",
    "cm = CausalModel(\n",
    "    Y=final_df[\"Y\"].values, \n",
    "    D=final_df[\"T\"].values, \n",
    "    X=features_df.values)\n",
    "\n",
    "cm.est_via_matching()\n",
    "# cm.est_via_ols()\n",
    "\n",
    "print(cm.estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalinference import CausalModel\n",
    "\n",
    "cm = CausalModel(\n",
    "    Y=final_df[\"Y\"].values, \n",
    "    D=final_df[\"T\"].values, \n",
    "    X=features_df.values)\n",
    "\n",
    "# cm.est_propensity_s()\n",
    "cm.est_via_matching()\n",
    "# cm.est_via_ols()\n",
    "\n",
    "print(cm.estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_features = [feature for feature in list(features_df) if not str(feature).startswith(\"bio2018\")]\n",
    "wanted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalinference import CausalModel\n",
    "\n",
    "prev_data_D_Y = None\n",
    "prev_data_features = None\n",
    "lectures = sorted(final_df[\"lecture\"].unique().tolist())\n",
    "results2 = []\n",
    "for lecture in lectures:\n",
    "    print(f\"Lecture: {lecture}\")\n",
    "    \n",
    "    if prev_data_D_Y is not None:\n",
    "        prev_data_D_Y = pd.concat([prev_data_D_Y, final_df[final_df[\"lecture\"]==lecture]], axis=0)\n",
    "        prev_data_features = pd.concat([prev_data_features, features_df[features_df[lecture]==1][wanted_features]],axis=0)\n",
    "        \n",
    "    else:\n",
    "        prev_data_D_Y = final_df[final_df[\"lecture\"]==lecture]\n",
    "        prev_data_features = features_df[features_df[lecture]==1][wanted_features]\n",
    "        \n",
    "    print(prev_data_features.shape)\n",
    "    if lecture==\"bio2018_lecture01\" or lecture==\"bio2018_lecture02\":\n",
    "        continue\n",
    "    cm = CausalModel(\n",
    "        Y=prev_data_D_Y[\"Y\"].values, \n",
    "        D=prev_data_D_Y[\"T\"].values, \n",
    "        X=prev_data_features.values)\n",
    "\n",
    "    cm.est_via_matching()\n",
    "#     cm.est_propensity_s()\n",
    "#     cm.est_via_ols()\n",
    "\n",
    "\n",
    "    print(cm.estimates)\n",
    "#     results2.append([\"OLS\", lecture, cm.estimates[\"ols\"][\"att\"], cm.estimates[\"ols\"][\"att_se\"]])\n",
    "    results2.append([\"Matching\",lecture, cm.estimates[\"matching\"][\"att\"], cm.estimates[\"matching\"][\"att_se\"]])\n",
    "\n",
    "#     print(cm.estimates[\"matching\"][\"att\"], cm.estimates[\"matching\"][\"att_se\"])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results2, columns=[\"Method\",\"Lecture\",\"ATT\",\"S.e\"]).to_csv(\"Lectures_results_matching.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "  \n",
    " - Calc propensity score\n",
    " \n",
    " - Calc IPSW\n",
    " \n",
    " - Calc S-learner\n",
    " \n",
    " - Calc T-learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get S-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "import numpy as np\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "from sklearn import feature_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import LeavePOut\n",
    "\n",
    "est = LinearRegression()\n",
    "pipeline = Pipeline(\n",
    "        [\n",
    "#          ('scale', scaler),\n",
    "#          ('select', selector),\n",
    "         ('classifier', est)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1234\n",
    "outer_folds = 10\n",
    "outer_cv = KFold(n_splits=outer_folds, shuffle=True, random_state=random_seed)\n",
    "\n",
    "# Prepare data\n",
    "X = pd.concat([features_df, final_df[\"T\"]], axis=1)\n",
    "y = final_df[\"Y\"]\n",
    "wanted_columns = list(X)\n",
    "X[\"S_learner_0\"] = 0\n",
    "X[\"S_learner_1\"] = 0\n",
    "\n",
    "# Fit\n",
    "pipeline.fit(X,y)\n",
    "\n",
    "# Predict\n",
    "X[\"T\"] = 0\n",
    "y_pred_0 = pipeline.predict(X)\n",
    "X[\"T\"] = 1\n",
    "y_pred_1 = pipeline.predict(X)\n",
    "\n",
    "# Save results\n",
    "X[\"S_learner_0\"] = y_pred_0\n",
    "X[\"S_learner_1\"] = y_pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_copy = final_df.copy()\n",
    "data1_copy[\"S_learner_0\"] = X[\"S_learner_0\"]\n",
    "data1_copy[\"S_learner_1\"] = X[\"S_learner_1\"]\n",
    "ATT_data1_S_learner = (data1_copy[data1_copy[\"T\"]==1][\"S_learner_1\"] - data1_copy[data1_copy[\"T\"]==1][\"S_learner_0\"]).mean()\n",
    "ATT_data1_S_learner_std = (data1_copy[data1_copy[\"T\"]==1][\"S_learner_1\"] - data1_copy[data1_copy[\"T\"]==1][\"S_learner_0\"]).std()\n",
    "print(\"ATT + std\")\n",
    "ATT_data1_S_learner, ATT_data1_S_learner_std / data1_copy[data1_copy[\"T\"]==1].shape[0]**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return round(m, 3), round(m-h, 3), round(m+h, 3)\n",
    "print(\"ATT + Confidence intervals:\")\n",
    "mean_confidence_interval((data1_copy[data1_copy[\"T\"]==1][\"S_learner_1\"] - data1_copy[data1_copy[\"T\"]==1][\"S_learner_0\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get T-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer, PowerTransformer\n",
    "\n",
    "est = Lasso(alpha=0.1)\n",
    "# scaler = QuantileTransformer(output_distribution='uniform')\n",
    "# scaler = StandardScaler()\n",
    "# selector = feature_selection.RFE(est, 15)\n",
    "pipeline_0 = Pipeline(\n",
    "        [\n",
    "#          ('scale', scaler),\n",
    "#          ('select', selector),\n",
    "         ('classifier', est)])\n",
    "\n",
    "est2 = Lasso(alpha=0.1)\n",
    "# scaler2 = QuantileTransformer(output_distribution='uniform')\n",
    "# selector2 = feature_selection.RFE(est, 15)\n",
    "pipeline_1 = Pipeline(\n",
    "        [\n",
    "#          ('scale', scaler2),\n",
    "#          ('select', selector2),\n",
    "         ('classifier', est2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "import numpy as np\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "from sklearn import feature_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import LeavePOut\n",
    "\n",
    "random_seed = 1234\n",
    "outer_folds = 10\n",
    "outer_cv = KFold(n_splits=outer_folds, shuffle=True, random_state=random_seed)\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "X = pd.concat([features_df, final_df[\"T\"]], axis=1)\n",
    "y = final_df[\"Y\"]\n",
    "wanted_columns = list(X)\n",
    "X[\"S_learner_0\"] = 0\n",
    "X[\"S_learner_1\"] = 0\n",
    "\n",
    "X_controle = X[X[\"T\"]==0][wanted_columns]\n",
    "X_treatment = X[X[\"T\"]==1][wanted_columns]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit\n",
    "pipeline_0 = pipeline_0.fit(X=X_controle, y=y.iloc[X_controle.index])\n",
    "pipeline_1 = pipeline_1.fit(X=X_treatment, y=y.iloc[X_treatment.index])\n",
    "\n",
    "# Predict\n",
    "y_pred_0 = pipeline_0.predict(X[wanted_columns])\n",
    "y_pred_1 = pipeline_1.predict(X[wanted_columns])\n",
    "   \n",
    "    \n",
    "# Save results\n",
    "X[\"S_learner_0\"] = y_pred_0\n",
    "X[\"S_learner_1\"] = y_pred_1    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_copy = final_df.copy()\n",
    "data1_copy[\"S_learner_0\"] = X[\"S_learner_0\"]\n",
    "data1_copy[\"S_learner_1\"] = X[\"S_learner_1\"]\n",
    "ATT_data1_S_learner = (data1_copy[data1_copy[\"T\"]==1][\"S_learner_1\"] - data1_copy[data1_copy[\"T\"]==1][\"S_learner_0\"]).mean()\n",
    "ATT_data1_S_learner_std = (data1_copy[data1_copy[\"T\"]==1][\"S_learner_1\"] - data1_copy[data1_copy[\"T\"]==1][\"S_learner_0\"]).std()\n",
    "print(\"ATT + std\")\n",
    "ATT_data1_S_learner, ATT_data1_S_learner_std/data1_copy[data1_copy[\"T\"]==1].shape[0]**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return f\"{round(m, 3)} ({round(m-h, 3)}, {round(m+h, 3)})\"\n",
    "print(\"ATT + Confidence intervals:\")\n",
    "mean_confidence_interval((data1_copy[data1_copy[\"T\"]==1][\"S_learner_1\"] - data1_copy[data1_copy[\"T\"]==1][\"S_learner_0\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=1000)\n",
    "# X = pd.concat([features_df, final_df[\"T\"]], axis=1)\n",
    "# knn = KNeighborsRegressor(n_neighbors=100, algorithm='brute', metric='mahalanobis', metric_params={'V': np.cov(X)})\n",
    "\n",
    "est = Lasso(alpha=0.1)\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# scaler = QuantileTransformer(output_distribution='uniform')\n",
    "# selector = feature_selection.RFE(est, 15)\n",
    "pipeline = Pipeline(\n",
    "        [\n",
    "#          ('scale', scaler),\n",
    "#          ('select', selector),\n",
    "         ('classifier', est)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "import numpy as np\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "from sklearn import feature_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import LeavePOut\n",
    "\n",
    "random_seed = 1234\n",
    "outer_folds = 10\n",
    "outer_cv = KFold(n_splits=outer_folds, shuffle=True, random_state=random_seed)\n",
    "\n",
    "# Prepare data\n",
    "X = pd.concat([features_df, final_df[\"T\"]], axis=1)\n",
    "y = final_df[\"Y\"]\n",
    "wanted_columns = list(X)\n",
    "X[\"Matched_y\"] = 0\n",
    "X[\"predicted_y\"] = 0\n",
    "\n",
    "# Fit\n",
    "pipeline.fit(X=X[wanted_columns], y=y)\n",
    "knn.fit(X[wanted_columns], y)\n",
    "\n",
    "# Predict \n",
    "X[\"predicted_y\"] = pipeline.predict(X[wanted_columns])\n",
    "distances, indices = knn.kneighbors(X[wanted_columns])\n",
    "for i,v in enumerate(list(X.index)):\n",
    "    print(i)\n",
    "    curr_T = X.loc[i,\"T\"]\n",
    "    neighbors = X.iloc[indices[i]]\n",
    "    nn = list(neighbors[neighbors[\"T\"] !=curr_T].index)[0]\n",
    "    X.loc[i,\"Matched_y\"] = X.loc[i,\"predicted_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_copy = final_df.copy()\n",
    "data1_copy[\"Matched_y\"] = X[\"Matched_y\"]\n",
    "ATT_data2_T_learner = (data1_copy[data1_copy[\"T\"]==1][\"Y\"] - data1_copy[data1_copy[\"T\"]==1][\"Matched_y\"]).mean()\n",
    "ATT_data1_S_learner_std = (data1_copy[data1_copy[\"T\"]==1][\"Y\"] - data1_copy[data1_copy[\"T\"]==1][\"Matched_y\"]).std()\n",
    "print(\"ATT + std\")\n",
    "ATT_data2_T_learner, ATT_data1_S_learner_std / data1_copy[data1_copy[\"T\"]==1].shape[0]**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "print(\"ATT + Confidence intervals:\")\n",
    "mean_confidence_interval((data1_copy[data1_copy[\"T\"]==1][\"Y\"] - data1_copy[data1_copy[\"T\"]==1][\"Matched_y\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get IPSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_auc_curve(model, X, y, is_plot_show=True):\n",
    "    preds = model.predict_proba(X)\n",
    "    preds_classes = model.predict(X)\n",
    "    preds = preds[:, 1]\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y, preds)\n",
    "    # calculate F1 score\n",
    "    f1 = f1_score(y, preds_classes)\n",
    "    # calculate precision-recall AUC\n",
    "    auc_score = auc(recall, precision)\n",
    "    # calculate average precision score\n",
    "    ap = average_precision_score(y, preds)\n",
    "    print('f1=%.3f auc=%.3f ap=%.3f' % (f1, auc_score, ap))\n",
    "    # plot no skill\n",
    "    if is_plot_show:\n",
    "        pyplot.plot([0, 1], [0.1, 0.1], '-b', linestyle='--')\n",
    "        # plot the precision-recall curve for the model\n",
    "        pyplot.plot(recall, precision, '-b', marker='.', label=\"PR-curve=%f\"%(auc_score))\n",
    "        pyplot.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=1)\n",
    "\n",
    "\n",
    "\n",
    "        # show the plot\n",
    "        pyplot.title(\"Curves\")\n",
    "        pyplot.show()\n",
    "\n",
    "    return f1, auc_score\n",
    "\n",
    "def get_conf_matrix(model, X, y):\n",
    "    preds = model.predict(X)\n",
    "    clf_report = classification_report(y, preds, target_names=[\"0\", \"1\"])\n",
    "    print(clf_report)\n",
    "    conf_mat = confusion_matrix(y, preds, labels=[0, 1])\n",
    "    print(conf_mat)\n",
    "    print(\"F1 score:\", f1_score(y, preds))\n",
    "    return conf_mat, clf_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learn the best model for propensity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n",
    "#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n",
    "# License: BSD Style.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer, PowerTransformer\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "import numpy as np\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "from sklearn import feature_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "# Create dataset of classification task with many redundant and few\n",
    "# informative features\n",
    "X = features_df.copy()\n",
    "y = final_df[\"T\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,\n",
    "                                                    random_state=42)\n",
    "\n",
    "\n",
    "def plot_calibration_curve(est, name, fig_index):\n",
    "    \"\"\"Plot calibration curve for est w/o and with calibration. \"\"\"\n",
    "    # Calibrated with isotonic calibration\n",
    "    isotonic = CalibratedClassifierCV(est, cv=2, method='isotonic')\n",
    "\n",
    "    # Calibrated with sigmoid calibration\n",
    "    sigmoid = CalibratedClassifierCV(est, cv=2, method='sigmoid')\n",
    "\n",
    "    # Logistic regression with no calibration as baseline\n",
    "\n",
    "    fig = plt.figure(fig_index, figsize=(10, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    for clf, name in [(est, name),\n",
    "                      (isotonic, name + ' + Isotonic'),\n",
    "                      (sigmoid, name + ' + Sigmoid')]:\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            prob_pos = clf.predict_proba(X_test)[:, 1]\n",
    "        else:  # use decision function\n",
    "            prob_pos = clf.decision_function(X_test)\n",
    "            prob_pos = \\\n",
    "                (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "\n",
    "        clf_score = brier_score_loss(y_test, prob_pos, pos_label=y.max())\n",
    "        print(\"%s:\" % name)\n",
    "        print(\"\\tBrier: %1.3f\" % (clf_score))\n",
    "        print(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\n",
    "        print(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\n",
    "        print(\"\\tF1: %1.3f\\n\" % f1_score(y_test, y_pred))\n",
    "        get_conf_matrix(clf, X_test, y_test)\n",
    "        \n",
    "        fraction_of_positives, mean_predicted_value = \\\n",
    "            calibration_curve(y_test, prob_pos, n_bins=10)\n",
    "\n",
    "        ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
    "                 label=\"%s (%1.3f)\" % (name, clf_score))\n",
    "\n",
    "        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n",
    "                 histtype=\"step\", lw=2)\n",
    "\n",
    "    ax1.set_ylabel(\"Fraction of positives\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_title('Calibration plots  (reliability curve)')\n",
    "\n",
    "    ax2.set_xlabel(\"Mean predicted value\")\n",
    "    ax2.set_ylabel(\"Count\")\n",
    "    ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    \n",
    "# Plot calibration curve for Gaussian Naive Bayes\n",
    "est = GaussianNB()\n",
    "scaler =  StandardScaler()\n",
    "pipeline = Pipeline(\n",
    "        [\n",
    "#            ('scale', scaler),\n",
    "#          ('select', selector),\n",
    "         ('classifier', est)])\n",
    "plot_calibration_curve(pipeline, \"Naive Bayes\", 1)\n",
    "\n",
    "est = LinearSVC(max_iter=10000)\n",
    "# scaler = QuantileTransformer(output_distribution='uniform')\n",
    "scaler =  StandardScaler()\n",
    "# selector = feature_selection.RFE(est, 5)\n",
    "pipeline = Pipeline(\n",
    "        [\n",
    "#          ('scale', scaler),\n",
    "#          ('select', selector),\n",
    "         ('classifier', est)])\n",
    "# Plot calibration curve for Linear SVC\n",
    "plot_calibration_curve(pipeline, \"SVC\", 2)\n",
    "\n",
    "est = LogisticRegression(C=1., solver=\"lbfgs\", penalty='none')\n",
    "scaler =  StandardScaler()\n",
    "# selector = feature_selection.RFE(est, 10)\n",
    "pipeline = Pipeline(\n",
    "        [\n",
    "#         ('scale', scaler),\n",
    "#          ('select', selector),\n",
    "         ('classifier', est)])\n",
    "# Plot calibration curve for Linear SVC\n",
    "plot_calibration_curve(pipeline, \"LR\", 3)\n",
    "\n",
    "est = tree.DecisionTreeClassifier(max_depth=6, class_weight={0:1, 1:1})\n",
    "pipeline = Pipeline(\n",
    "        [\n",
    "         ('classifier', est)])\n",
    "# Plot calibration curve for Linear SVC\n",
    "plot_calibration_curve(pipeline, \"DT\", 4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick the best model for propensity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = LogisticRegression(C=1., solver=\"lbfgs\", penalty='none')\n",
    "pipeline = Pipeline(\n",
    "        [\n",
    "#         ('scale', scaler),\n",
    "#          ('select', selector),\n",
    "         ('classifier', est)])\n",
    "calibrator = CalibratedClassifierCV(pipeline, cv='prefit', method='sigmoid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict propensity score ove the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"T\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "import numpy as np\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "from sklearn import feature_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "random_seed = 1234\n",
    "outer_folds = 10\n",
    "# outer_cv = KFold(n_splits=outer_folds, shuffle=True, random_state=random_seed)\n",
    "outer_cv = StratifiedKFold(n_splits=outer_folds, shuffle=True, random_state=random_seed)\n",
    "\n",
    "X = features_df.copy()\n",
    "y = final_df[\"T\"]\n",
    "\n",
    "pipeline.fit(X,y)\n",
    "calibrator.fit(X,y)\n",
    "\n",
    "\n",
    "# calibrator_preds = calibrator.predict_proba(X)[:,1]\n",
    "\n",
    "# pipeline_preds = pipeline.predict_proba(X)[:,1]\n",
    "\n",
    "propensity_pipeline = pd.DataFrame(pipeline.predict_proba(X))\n",
    "propensity_calibrator = pd.DataFrame(calibrator.predict_proba(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propensity score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Propensity score from pipeline: {brier_score_loss(y, propensity_pipeline.loc[:,1], pos_label=y.max())}\")\n",
    "print(f\"Propensity score from calibrator: {brier_score_loss(y, propensity_calibrator.loc[:,1], pos_label=y.max())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calc IPSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_copy = final_df.copy()\n",
    "# lg = LogisticRegression()\n",
    "lg = LogisticRegression(C=1., solver=\"lbfgs\", penalty='l2')\n",
    "X = features_df\n",
    "y = final_df[\"T\"]\n",
    "lg.fit(X,y)\n",
    "\n",
    "propensity = lg.predict_proba(X)[:,1]\n",
    "\n",
    "final_df_copy[\"propensity1\"] = propensity\n",
    "final_df_copy[\"propensity2\"] = 1/propensity\n",
    "final_df_copy[\"propensity3\"] = propensity/(1-propensity)\n",
    "\n",
    "final_df_copy[\"w_t\"] = final_df_copy.apply(lambda row: row[\"T\"]+((1-row[\"T\"])*row[\"propensity1\"])/(1-row[\"propensity1\"]), axis=1).clip(-10,10)\n",
    "final_df_copy[\"score\"] = final_df_copy[\"Y\"]*final_df_copy[\"w_t\"]\n",
    "\n",
    "final_df_copy[\"ips\"] = np.where(\n",
    "    final_df[\"T\"] == 1, \n",
    "    1 / propensity,\n",
    "    1 / (1 - propensity))\n",
    "\n",
    "final_df_copy[\"ipsw\"] = final_df_copy[\"Y\"] * final_df_copy.ips\n",
    "\n",
    "# Trip too big inverse propensity scores\n",
    "print(final_df_copy.shape)\n",
    "final_df_copy = final_df_copy[final_df_copy[\"ipsw\"]<30]\n",
    "print(final_df_copy.shape)\n",
    "\n",
    "\n",
    "ipse = (\n",
    "      final_df_copy[final_df_copy[\"T\"] == 1][\"ipsw\"].sum() \n",
    "    - final_df_copy[final_df_copy[\"T\"] == 0][\"ipsw\"].sum()\n",
    ") / final_df_copy.shape[0]\n",
    "\n",
    "print(\"ATE:\", ipse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above approach is similar to:\n",
    "Y1 = final_df_copy[final_df_copy[\"T\"] == 1][\"ipsw\"].sum() / final_df_copy.shape[0]\n",
    "Y0 = final_df_copy[final_df_copy[\"T\"] == 0][\"ipsw\"].sum() / final_df_copy.shape[0]\n",
    "print(\"ATE:\", Y1-Y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_copy.apply(lambda x: x[\"w_t\"]*x[\"Y\"],axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "# Taken from here: https://onlinelibrary.wiley.com/doi/epdf/10.1002/bimj.201600094\n",
    "Image(filename='IPW formula.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1_a = final_df_copy[final_df_copy[\"T\"] == 1][\"Y\"].sum() \n",
    "Y1_b =  final_df_copy[final_df_copy[\"T\"] == 1][\"Y\"].shape[0]\n",
    "\n",
    "Y0_a = (final_df_copy[final_df_copy[\"T\"] == 0][\"Y\"] * final_df_copy[final_df_copy[\"T\"] == 0][\"w_t\"]).sum()\n",
    "Y0_b = (final_df_copy[final_df_copy[\"T\"] == 0][\"propensity3\"]).sum()\n",
    "ipsw_ATT = Y1_a/Y1_b - Y0_a/Y0_b\n",
    "print(\"ATT:\", ipsw_ATT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_copy[final_df_copy[\"T\"] == 1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_copy[\"att\"] = ipsw_ATT\n",
    "Y1_a = (final_df_copy[final_df_copy[\"T\"] == 1][\"Y\"] - final_df_copy[final_df_copy[\"T\"] == 1][\"att\"]).sum()\n",
    "Y1_b =  final_df_copy[final_df_copy[\"T\"] == 1][\"Y\"].shape[0]\n",
    "\n",
    "Y0_a = (final_df_copy[final_df_copy[\"T\"] == 0][\"Y\"] * final_df_copy[final_df_copy[\"T\"] == 0][\"w_t\"] - final_df_copy[final_df_copy[\"T\"] == 0][\"att\"]).sum()\n",
    "Y0_b = (final_df_copy[final_df_copy[\"T\"] == 0][\"propensity3\"]).sum()\n",
    "ipsw_ATT_std = Y1_a/Y1_b - Y0_a/Y0_b\n",
    "print(\"ATT_standard_error:\", ipsw_ATT_std/final_df_copy.shape[0]**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mean_confidence_interval2(mean, std,n, confidence=0.95):\n",
    "#     n = n\n",
    "#     m, se = mean, std\n",
    "#     h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "#     return round(m, 3), round(m-h, 3), round(m+h, 3)\n",
    "# mean_confidence_interval2(ipsw_ATT, ipwt_std, final_df_copy.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "read this:\n",
    "http://www.degeneratestate.org/posts/2018/Mar/24/causal-inference-with-python-part-1-potential-outcomes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "372.344px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
